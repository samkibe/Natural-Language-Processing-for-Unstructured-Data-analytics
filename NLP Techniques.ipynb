{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "256fa70c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
 
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77961912",
   "metadata": {},
   "source": [
    "## NLP Natural Language Processing\n",
    "### 1.Tokenization\n",
    "* Tokens, Keywords, Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b3572b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'work', 'and', 'no', 'play', 'makes', 'jack', 'a', 'dull', 'boy', ',', 'all', 'work', 'and', 'no', 'play']\n"
     ]
    }
   ],
   "source": [
    "#import nltk, word_tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize # this line is iport it initializes stuff\n",
    "data = \"All work and no play makes jack a dull boy, all work and no play\"\n",
    "words = word_tokenize(data)\n",
    "\n",
    "print (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1de162d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All work and no play makes jack a dull boy.', 'All work and no play.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "data = \"All work and no play makes jack a dull boy. All work and no play.\"\n",
    "words = sent_tokenize(data)\n",
    "print (words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b0cfb8",
   "metadata": {},
   "source": [
    "### 2. Model : creating bag of words \n",
    "* Steps 1. tokenize text, 2. create vocabulary, 3. count number of occurences, 4. Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6a77ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "key : ['it', 'was', 'the', 'of', 'times', 'age', 'best', 'worst', 'wisdom', 'foolishness']\n",
      "[[0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0.]\n",
      " [0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# 1. libraries\n",
    "from keras.preprocessing.text import Tokenizer # define four docs\n",
    "\n",
    "# 2. define corpus\n",
    "docs = [\"It was the best of times\",\n",
    "       \"It was the worst of times\",\n",
    "       \"It was the age of wisdom\",\n",
    "       \"It was the age of foolishness\"]\n",
    "\n",
    "# 3. Tokenize corpus\n",
    "# create tokenizer model\n",
    "model = Tokenizer() # made some changes ici lacked - ()\n",
    "\n",
    "# use model to create model\n",
    "Tokens = model.fit_on_texts(docs)\n",
    "print(Tokens)\n",
    "\n",
    "# 4. Define bag of words\n",
    "# display words\n",
    "print (f'key : {list(model.word_index.keys())}')\n",
    "\n",
    "# display frequency fro each word\n",
    "rep = model.texts_to_matrix(docs, mode = 'count')\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e5dce2",
   "metadata": {},
   "source": [
    "### stemming \n",
    "* to remove prefixes and suffixes on a word to remain with only a stem, this increases accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "868b6e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "programs : program\n",
      "programming : program\n",
      "Terrors : terror\n",
      "Terrorists : terrorist\n",
      "Terrorism : terror\n",
      "Modelling : model\n",
      "Networking : network\n"
     ]
    }
   ],
   "source": [
    "# libraries \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# use lib creates stemming obj\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# example - words to stem\n",
    "words = [\"programs\", \"programming\", \"Terrors\", \"Terrorists\", \"Terrorism\", \"Modelling\", \"Networking\"]\n",
    "\n",
    "# apply stemming objects to extract base words\n",
    "for w in words:\n",
    "    print(w, \":\", ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0724e23f",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "* it extracts a word as it would appear in the dictionary from a conjugated word etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f90ad1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "studies : study\n",
      "better : good\n",
      "worse : bad\n"
     ]
    }
   ],
   "source": [
    "# libraries \n",
    "from nltk.stem import WordNetLemmatizer # installed wordnet\n",
    "\n",
    "# define obj\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "# use the object to lemmatize words\n",
    "print(\"rocks :\", wl.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", wl.lemmatize(\"corpora\"))\n",
    "print(\"studies :\", wl.lemmatize(\"studies\")) # extra training word\n",
    "\n",
    "\n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", wl.lemmatize(\"better\", pos = \"a\"))\n",
    "print(\"worse :\", wl.lemmatize(\"worse\", pos = \"a\")) # extra training adjective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cac54c",
   "metadata": {},
   "source": [
    "### POS - part-of-speech tagging \n",
    "* defines the meaning of a sentence based on the contecxt, extracts relations between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5174a6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Today', 'NN'), ('morning', 'NN'), ('.', '.'), ('Arthur', 'NNP'), ('felt', 'VBD'), ('very', 'RB'), ('good', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "# input text\n",
    "text = \"\"\"Today morning. Arthur felt very good.\"\"\"\n",
    "\n",
    "# tokenize text into words \n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# apply POS tagging Function\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "# print agged tokens \n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f56520b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
